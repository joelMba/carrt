{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "726b9855",
   "metadata": {},
   "source": [
    "# Modèle carrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc768e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import carrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a50ac294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module carrt:\n",
      "\n",
      "NAME\n",
      "    carrt\n",
      "\n",
      "DESCRIPTION\n",
      "    CARRT (Classification based on Association Rules Ranking with Topsis Method) \n",
      "    is an associative classification framework using the 'TopKClassRules' algorithm for association rule mining. \n",
      "    It applies multicriteria analysis, particularly the TOPSIS method, for ranking association rules.\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        CARRT\n",
      "    \n",
      "    class CARRT(builtins.object)\n",
      "     |  CARRT(k, min_confidence, consequent_item_ids, number_of_rules_per_class)\n",
      "     |  \n",
      "     |  Usage:\n",
      "     |  Import the 'carrt' module, then create an instance of the 'CARRT' class after completing preprocessing and data preparation.\n",
      "     |  Inputs must use the Spmf format.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, k, min_confidence, consequent_item_ids, number_of_rules_per_class)\n",
      "     |      Initialize CARRT with parameters:\n",
      "     |      - k: Number of association rules to discover (positive integer).\n",
      "     |      - min_confidence: Minimum confidence for association rules (value in [0,1]).\n",
      "     |      - consequent_item_ids: Target class item(s) allowed in the rule consequent.\n",
      "     |      - number_of_rules_per_class: Number of rules per class for classifier construction.\n",
      "     |  \n",
      "     |  evaluation(self, test_data, dataset_name)\n",
      "     |      Evaluates the model on test data.\n",
      "     |      Inputs:\n",
      "     |      - test_data: DataFrame with test data.\n",
      "     |      - dataset_name: Name of the dataset.\n",
      "     |      \n",
      "     |      Outputs:\n",
      "     |      - Evaluation metrics: accuracy, precision, recall, f1-score.\n",
      "     |      - Confusion matrix.\n",
      "     |  \n",
      "     |  fit(self, data_train, data_train_structured_version)\n",
      "     |      Constructs the classifier from training data.\n",
      "     |      Inputs:\n",
      "     |      - data_train: Path to training data in spmf format (.txt).\n",
      "     |      - data_train_structured_version: Structured version of training data (dataframe) for calculating additional rule quality metrics.\n",
      "     |      \n",
      "     |      Outputs:\n",
      "     |      - self.classifier: DataFrame with the best rules for each class.\n",
      "     |      - self.train_result: Summary of training data results.\n",
      "     |      \n",
      "     |      Note: The target class should be explicitly named \"Class\".\n",
      "     |  \n",
      "     |  predict(self, test_data)\n",
      "     |      Predicts classes for test data.\n",
      "     |      Input:\n",
      "     |      - test_data: DataFrame containing the test data.\n",
      "     |      \n",
      "     |      Output:\n",
      "     |      - DataFrame with predicted values for each instance in the test set.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FILE\n",
      "    c:\\users\\joel.mba-kouhoue\\ma_these\\carrt\\carrt.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(carrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379e4d1f",
   "metadata": {},
   "source": [
    "# Préprocessing des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aa8bd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1060792b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module Preprocessing:\n",
      "\n",
      "NAME\n",
      "    Preprocessing - Module for preprocessing functions to prepare data for analysis, including transformation, filtering, and categorization.\n",
      "\n",
      "FUNCTIONS\n",
      "    data_preprocessing_old(df)\n",
      "        Preprocesses input data by filtering and transforming it for analysis.\n",
      "        Steps:\n",
      "            - Filters closed interventions.\n",
      "            - Selects and renames relevant columns.\n",
      "            - Handles missing values in numerical and non-numerical columns.\n",
      "            - Converts durations to hours and categorizes them symbolically.\n",
      "            - Processes event history for frequent events.\n",
      "            - Removes unnecessary columns and duplicates.\n",
      "        Arguments:\n",
      "            df : DataFrame - Input data containing intervention records.\n",
      "        Returns:\n",
      "            DataFrame - Transformed and cleaned data.\n",
      "    \n",
      "    symbolic_representation(time_duration)\n",
      "        Symbolically represents a duration in hours.\n",
      "        Arguments:\n",
      "            time_duration : float - Duration in hours.\n",
      "        Returns:\n",
      "            str - Symbolic category for the duration.\n",
      "\n",
      "FILE\n",
      "    c:\\users\\joel.mba-kouhoue\\ma_these\\carrt\\preprocessing.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357b5dd3",
   "metadata": {},
   "source": [
    "# Prépraparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35e2f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87ea1e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module Preparation:\n",
      "\n",
      "NAME\n",
      "    Preparation - This module contains functions for preparing data and converting it into the SPMF input format for machine learning algorithms.\n",
      "\n",
      "FUNCTIONS\n",
      "    data_conversion_to_spmf_txt(df, output_path)\n",
      "        Converts preprocessed intervention/operation data into SPMF format.\n",
      "        Arguments:\n",
      "            df : DataFrame - The preprocessed data.\n",
      "            output_path : str - The file path to save the transformed data.\n",
      "    \n",
      "    encode_data(df, encoding_dict)\n",
      "        Replaces values in the DataFrame with their corresponding codes.\n",
      "        Arguments:\n",
      "            df : DataFrame - The data to encode.\n",
      "            encoding_dict : dict - The mapping of events to codes.\n",
      "        Returns:\n",
      "            DataFrame - The encoded DataFrame.\n",
      "    \n",
      "    encode_elements(elements)\n",
      "        Encodes events into numeric format.\n",
      "        Arguments:\n",
      "            elements : list - A list of events to encode.\n",
      "        Returns:\n",
      "            dict - A dictionary mapping each event to a unique numeric code.\n",
      "    \n",
      "    export_data_to_spmf_txt(df, encoding_dict, output_path)\n",
      "        Exports encoded data to an SPMF-compatible text file.\n",
      "        Arguments:\n",
      "            df : DataFrame - The encoded data.\n",
      "            encoding_dict : dict - The mapping of events to codes.\n",
      "            output_path : str - The path to save the SPMF file.\n",
      "    \n",
      "    get_distinct_elements(df)\n",
      "        Creates a list of unique elements from the DataFrame.\n",
      "        Arguments:\n",
      "            df : DataFrame - The data.\n",
      "        Returns:\n",
      "            list - A list of distinct elements across all columns.\n",
      "    \n",
      "    remove_false_items(file_path)\n",
      "        Removes \"False\" items from the SPMF file by updating both item definitions and sequences.\n",
      "        Arguments:\n",
      "            file_path : str - The path to the SPMF file to modify.\n",
      "\n",
      "FILE\n",
      "    c:\\users\\joel.mba-kouhoue\\ma_these\\carrt\\preparation.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Preparation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47076066",
   "metadata": {},
   "source": [
    "# Post-traitement des règles d'association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb273749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Postraitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caed79f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module Postraitement:\n",
      "\n",
      "NAME\n",
      "    Postraitement\n",
      "\n",
      "DESCRIPTION\n",
      "    This module implements post-processing functions for association rules generated by the \n",
      "    TopKClassAssociationRules algorithm. It includes functions to remove conflicting or redundant rules, \n",
      "    compute additional metrics like lift and conviction, and rank rules using the TOPSIS method.\n",
      "\n",
      "FUNCTIONS\n",
      "    antecedent_support(antecedent, data)\n",
      "        Calculates the support of a given antecedent in a dataset.\n",
      "        \n",
      "        Parameters:\n",
      "        - antecedent: String representing the antecedent.\n",
      "        - data: DataFrame containing the dataset.\n",
      "        \n",
      "        Returns:\n",
      "        - support: Support of the antecedent in the dataset.\n",
      "    \n",
      "    antecedent_to_dict(antecedent)\n",
      "        Converts an antecedent string to a dictionary of key-value pairs.\n",
      "        \n",
      "        Parameters:\n",
      "        - antecedent: String representing the antecedent.\n",
      "        \n",
      "        Returns:\n",
      "        - result: Dictionary of key-value pairs.\n",
      "    \n",
      "    calculate_lift_conviction(train_data, rules_df)\n",
      "        Computes lift and conviction metrics for the association rules.\n",
      "        \n",
      "        Parameters:\n",
      "        - train_data: DataFrame of training data.\n",
      "        - rules_df: DataFrame of association rules.\n",
      "        \n",
      "        Returns:\n",
      "        - rules_df: DataFrame with lift and conviction columns added.\n",
      "    \n",
      "    delete_conflicting_rules(rules)\n",
      "        Removes conflicting rules, i.e., rules with the same antecedent but different conclusions.\n",
      "        \n",
      "        Parameters:\n",
      "        - rules: DataFrame containing the association rules.\n",
      "        \n",
      "        Returns:\n",
      "        - filtered_rules: DataFrame without conflicting rules.\n",
      "    \n",
      "    delete_redundant_rules(input_file)\n",
      "        Removes redundant and conflicting rules from a text file containing extracted rules.\n",
      "        \n",
      "        Parameters:\n",
      "        - input_file: Path to the file containing the rules (format: .txt).\n",
      "        \n",
      "        Returns:\n",
      "        - df: DataFrame with non-redundant and non-conflicting rules.\n",
      "    \n",
      "    is_dict_subset(dict1, dict2)\n",
      "        Checks if dict2 is a subset of dict1.\n",
      "        \n",
      "        Parameters:\n",
      "        - dict1: Main dictionary.\n",
      "        - dict2: Dictionary to check.\n",
      "        \n",
      "        Returns:\n",
      "        - bool: True if dict2 is a subset of dict1, otherwise False.\n",
      "    \n",
      "    rank_rules_with_topsis(df)\n",
      "        Ranks association rules using the TOPSIS (Technique for Order of Preference by Similarity to Ideal Solution) method.\n",
      "        \n",
      "        Steps:\n",
      "        1. Adds an index column and calculates the number of items in each antecedent.\n",
      "        2. Defines the evaluation criteria for the rules.\n",
      "        3. Uses the TOPSIS method to rank the rules:\n",
      "           - Entropy method is applied to calculate weights for each criterion based on their variability.\n",
      "           - Criteria are normalized and signals (maximize/minimize) are set for each.\n",
      "           - The ranking is determined by the TOPSIS decision process.\n",
      "        \n",
      "        Parameters:\n",
      "        - df: DataFrame containing the association rules to be ranked.\n",
      "          Required columns: 'Support', 'Confidence', 'Lift', 'Conviction', 'Antecedent'.\n",
      "        \n",
      "        Returns:\n",
      "        - df: DataFrame with two additional columns:\n",
      "            - 'Score': The TOPSIS performance score for each rule.\n",
      "            - 'Rank': The rank of each rule based on the performance score.\n",
      "    \n",
      "    visualize_association_rules(df_rules, number_of_rules)\n",
      "        Plots a scatter plot of the rules based on support and confidence.\n",
      "        \n",
      "        Parameters:\n",
      "        - df_rules: DataFrame containing the rules.\n",
      "        - number_of_rules: Number of rules to visualize.\n",
      "\n",
      "DATA\n",
      "    LinearMinMax_ = 'LinearMinMax'\n",
      "    LinearSum_ = 'LinearSum'\n",
      "    MAX = 1\n",
      "    MIN = -1\n",
      "\n",
      "FILE\n",
      "    c:\\users\\joel.mba-kouhoue\\ma_these\\carrt\\postraitement.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Postraitement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f6df46",
   "metadata": {},
   "source": [
    "# Prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9c3e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c315df8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module Prediction:\n",
      "\n",
      "NAME\n",
      "    Prediction - # coding: utf-8\n",
      "\n",
      "FUNCTIONS\n",
      "    accuracy(y_test, y_pred)\n",
      "        # Metrics calculation functions\n",
      "    \n",
      "    antecedent_to_dict(antecedent)\n",
      "        Convert an antecedent string into a dictionary.\n",
      "        \n",
      "        Parameters:\n",
      "            antecedent (str): Antecedent string in 'key=value' format.\n",
      "        \n",
      "        Returns:\n",
      "            dict: Parsed antecedent as a dictionary.\n",
      "    \n",
      "    categorie2(df_rule_c, intervention, max_occurrence_value)\n",
      "        Determine predicted class based on candidate rules.\n",
      "        \n",
      "        Parameters:\n",
      "            df_rule_c (pd.DataFrame): Candidate rules.\n",
      "            intervention (dict): Intervention attributes.\n",
      "            max_occurrence_value: Default class in case of ambiguity.\n",
      "        \n",
      "        Returns:\n",
      "            pd.DataFrame: Intervention with predicted class.\n",
      "    \n",
      "    dict_subsetOf_dict(dict1, dict2)\n",
      "        Check if dict2 is a subset of dict1.\n",
      "        \n",
      "        Parameters:\n",
      "            dict1 (dict): Base dictionary.\n",
      "            dict2 (dict): Subset to check.\n",
      "        \n",
      "        Returns:\n",
      "            bool: True if dict2 is a subset of dict1, False otherwise.\n",
      "    \n",
      "    f1(y_test, y_pred)\n",
      "    \n",
      "    matching_Rule3(intervention, df_rules)\n",
      "        Match rules based on the attributes of a given intervention.\n",
      "        \n",
      "        Parameters:\n",
      "            intervention (dict): Dictionary of intervention attributes.\n",
      "            df_rules (pd.DataFrame): Rule set to match.\n",
      "        \n",
      "        Returns:\n",
      "            pd.DataFrame: DataFrame of matching rules.\n",
      "    \n",
      "    no_matching_rule(df, df_rules)\n",
      "        Assign random class to observations with no matching rules.\n",
      "        \n",
      "        Parameters:\n",
      "            df (pd.DataFrame): Data with predictions.\n",
      "            df_rules (pd.DataFrame): Rule set for reference.\n",
      "        \n",
      "        Returns:\n",
      "            pd.DataFrame: Data with predictions filled for unmatched cases.\n",
      "    \n",
      "    precision(y_test, y_pred)\n",
      "    \n",
      "    prediction3(df, df_rules)\n",
      "        Perform prediction using rule-based inference on the input DataFrame.\n",
      "        \n",
      "        Parameters:\n",
      "            df (pd.DataFrame): Input data for prediction. Must include the 'Class' column.\n",
      "            df_rules (pd.DataFrame): Rule set for inference.\n",
      "        \n",
      "        Returns:\n",
      "            pd.DataFrame: DataFrame with added predictions.\n",
      "    \n",
      "    print_confusion_matrix(y_test, y_pred)\n",
      "        Display confusion matrix as a heatmap.\n",
      "    \n",
      "    recall(y_test, y_pred)\n",
      "\n",
      "FILE\n",
      "    c:\\users\\joel.mba-kouhoue\\ma_these\\carrt\\prediction.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help((Prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3d345e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
